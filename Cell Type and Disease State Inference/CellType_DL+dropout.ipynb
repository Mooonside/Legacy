{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Previous Data!\n",
      "883\n",
      "(1077, 883) (1077, 1)\n",
      "19 883\n"
     ]
    }
   ],
   "source": [
    "MicroArray,Labels = read_data(mode = \"CellType\",th = 20)\n",
    "MicroArray = pca(MicroArray,ratio=0.95)\n",
    "print MicroArray.shape,Labels.shape\n",
    "\n",
    "class_num = len(np.unique(Labels))\n",
    "input_dim = MicroArray.shape[1]\n",
    "print class_num,input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"./res/ds_pca.npy\",MicroArray)\n",
    "# np.save(\"./res/ds_labels.npy\",Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 883)\n",
      "(208, 883)\n",
      "(869, 1)\n",
      "(208, 1)\n"
     ]
    }
   ],
   "source": [
    "## shuffle data and divide it into 5 parts\n",
    "from util import *\n",
    "\n",
    "MicroLabels = Labels\n",
    "MicroArray_train,MicroLabels_train,MicroArray_test,MicroLabels_test = division(MicroArray,MicroLabels)\n",
    "\n",
    "print MicroArray_train.shape\n",
    "print MicroArray_test.shape\n",
    "print MicroLabels_train.shape\n",
    "print MicroLabels_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# use tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def save_model(name):\n",
    "    saver = tf.train.Saver()\n",
    "    path = \"./models/%s.ckpt\" % name\n",
    "    save_path = saver.save(sess,path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "def load_model(name):\n",
    "    saver = tf.train.Saver()\n",
    "    path = \"./models/%s.ckpt\" % name\n",
    "    saver.restore(sess, path)\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating nodes for the input images and target output classes.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None,input_dim])\n",
    "y_ = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32,shape=[])\n",
    "\n",
    "reg_penalty = tf.placeholder(tf.float32,shape=[])\n",
    "\n",
    "starter_learning_rate = tf.placeholder(tf.float32,shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "W1 = tf.Variable(\n",
    "    tf.random_normal(shape = [input_dim,2000],mean=0.01,stddev=0.1),name = \"W1\"\n",
    ")\n",
    "b1 = tf.Variable(\n",
    "    tf.random_normal(shape = [2000],mean=0.01,stddev=0.1),name = \"b1\"\n",
    ")\n",
    "n1 = reg_penalty * (tf.reduce_sum(tf.square(W1)) + tf.reduce_sum(tf.square(b1)))\n",
    "\n",
    "y1 = tf.matmul(x,W1) + b1\n",
    "o1 = tf.nn.relu(y1)\n",
    "d1 = tf.nn.dropout(o1,keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(\n",
    "    tf.random_normal(shape = [2000,1000],mean=0.01,stddev=0.1),name = \"W2\"\n",
    ")\n",
    "b2 = tf.Variable(\n",
    "    tf.random_normal(shape = [1000],mean=0.01,stddev=0.1),name = \"b2\"\n",
    ")\n",
    "n2 = reg_penalty * (tf.reduce_sum(tf.square(W2)) + tf.reduce_sum(tf.square(b2)))\n",
    "\n",
    "y2 = tf.matmul(d1,W2) + b2\n",
    "o2 = tf.nn.relu(y2)\n",
    "d2 = tf.nn.dropout(o2,keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.Variable(\n",
    "    tf.random_normal(shape = [1000,500],mean=0.01,stddev=0.1),name = \"W3\"\n",
    ")\n",
    "b3 = tf.Variable(\n",
    "    tf.random_normal(shape = [500],mean=0.01,stddev=0.1),name = \"b3\"\n",
    ")\n",
    "n3 = reg_penalty * (tf.reduce_sum(tf.square(W3)) + tf.reduce_sum(tf.square(b3)))\n",
    "\n",
    "y3 = tf.matmul(d2,W3) + b3\n",
    "o3 = tf.nn.relu(y3)\n",
    "d3 = tf.nn.dropout(o3,keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "W4 = tf.Variable(\n",
    "    tf.random_normal(shape = [500,class_num],mean=0.01,stddev=0.1),name = \"W4\"\n",
    ")\n",
    "b4 = tf.Variable(\n",
    "    tf.random_normal(shape = [class_num],mean=0.01,stddev=0.1),name = \"b4\"\n",
    ")\n",
    "n4 = reg_penalty * (tf.reduce_sum(tf.square(W4)) + tf.reduce_sum(tf.square(b4)))\n",
    "\n",
    "y4 = tf.matmul(d3,W4) + b4\n",
    "\n",
    "\n",
    "onehot_labels = tf.one_hot(indices=tf.cast(y_, tf.int32), depth = class_num)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=onehot_labels, logits=y4))\n",
    "\n",
    "#loss = cross_entropy\n",
    "loss = cross_entropy + n1 + n2 + n3 + n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step_tensor,\n",
    "                                       10000, 0.96, staircase=True)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(\n",
    "    learning_rate = learning_rate,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.999,\n",
    "    epsilon = 1e-08,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "train_step = opt.minimize(loss,\n",
    "    global_step = global_step_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_accuracy(data,labels):\n",
    "    prediction = tf.identity(y4).eval(feed_dict={x: data,keep_prob:1})\n",
    "    prediction = np.argmax(prediction,axis=1).reshape([-1,1])\n",
    "    return np.mean(prediction == labels)\n",
    "\n",
    "def predict(data):\n",
    "    prediction = tf.identity(y4).eval(feed_dict={x: data,keep_prob:1})\n",
    "    prediction = np.argmax(prediction,axis=1)\n",
    "    return prediction\n",
    "    \n",
    "def eval_mse_loss(data,labels,reg):\n",
    "    entropy = tf.identity(cross_entropy).eval(feed_dict={x: data, y_: labels,reg_penalty:reg,keep_prob:1})\n",
    "    return entropy\n",
    "\n",
    "def eval_loss(data,labels,reg):\n",
    "    l = tf.identity(loss).eval(feed_dict={x: data, y_: labels,reg_penalty:reg,keep_prob:1})\n",
    "    return l\n",
    "\n",
    "def valid_batch(batch,th=0.2):\n",
    "    uni = np.unique(batch)\n",
    "    for i in uni:\n",
    "        if(np.sum(batch==i) > th*batch.shape[0]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "ct_lr_0.001000_l2_0.001000+dp_0.600000\n",
      "Model saved in file: ./models/ct_lr_0.001000_l2_0.001000+dp_0.600000.ckpt\n",
      "epoch:0   train loss:2353.953369 ,test loss :1464.682617,train accuracy:0.657077,test accuracy:0.600962\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-327bfa5bd0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mrandom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMicroArray_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mvalid_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mrandom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMicroArray_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-457cdbd71fc0>\u001b[0m in \u001b[0;36mvalid_batch\u001b[0;34m(batch, th)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalid_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0muni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0moptional_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "epoch = int( MicroArray_train.shape[0] / batch_size)\n",
    "print epoch\n",
    "epoch_iter = 500\n",
    "max_test = 0\n",
    "\n",
    "rp = 1e-3\n",
    "kp = 0.6\n",
    "sl = 1e-3\n",
    "\n",
    "modelname = \"ct_lr_%f_l2_%f+dp_%f\" % (sl,rp,kp)\n",
    "print modelname\n",
    "\n",
    "for step in range(epoch_iter * epoch):\n",
    "  random = np.random.choice(MicroArray_train.shape[0],batch_size,replace=False)\n",
    "  while(not valid_batch(Labels[random])):\n",
    "        random = np.random.choice(MicroArray_train.shape[0],batch_size,replace=False)\n",
    "      \n",
    "  feed_dict = {\n",
    "      x: MicroArray_train[random,:], \n",
    "      y_: MicroLabels_train[random],\n",
    "      reg_penalty:rp,\n",
    "      keep_prob:kp,\n",
    "      starter_learning_rate:sl\n",
    "    }\n",
    "\n",
    "  _,a = sess.run([train_step,loss],feed_dict=feed_dict)\n",
    "\n",
    "  if(step % epoch == 0):\n",
    "    train_loss.append(a)\n",
    "    b = eval_loss(MicroArray_test,MicroLabels_test,rp)\n",
    "    test_loss.append(b)\n",
    "\n",
    "    train_accuracy = eval_accuracy(MicroArray_train,MicroLabels_train)\n",
    "    test_accuracy = eval_accuracy(MicroArray_test,MicroLabels_test)\n",
    "    train_acc.append(train_accuracy)\n",
    "    test_acc.append(test_accuracy)\n",
    "    \n",
    "    if(test_accuracy > max_test):\n",
    "        max_test = test_accuracy\n",
    "        save_model(modelname)\n",
    "    print \"epoch:%d   train loss:%f ,test loss :%f,train accuracy:%f,test accuracy:%f\" % \\\n",
    "        (step/epoch,a,b,train_accuracy,test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model(modelname)\n",
    "print eval_accuracy(MicroArray_test,MicroLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss,\"b\")\n",
    "plt.plot(test_loss,\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc,\"b\")\n",
    "plt.plot(test_acc,\"g\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
